Voter Registration import process:
The Minnesota Secretary of State’s office will post data for us to their FTP server the first week of each quarter of the year – so the first week of January, first week of April, first week of July, and first week of October. 
We have to pay for these 4 data dumps in advance. Our first payment was beginning of Jan 2016 and that’s also when we first got the data through this route.
The FTP site is ftp.sos.state.mn.us   usually works best with FTP client software (wsftp, cuteftp, cyberduck, winscp, etc) 

       User: startribdata
       Password: TV1GhN32+eo3Q61
       (Leave the Port field blank in Filezilla)
       The files are encrypted and compressed, no client software is needed, the password for decrypting is 2016StarTribData 

There will be 18 compressed files.
--The 8 starting with “st_election” are for a voting history table (all files have same layout)
--The 8 starting with “st_voter” are for a voter registration table (all files have same layout)
--The other – ST_PFlist and ST_PPlist – will not be imported to Uniquery, but they are good to have on hand for other uses. So they should just be downloaded. 
The files are all comma-delimited with double quotes as text qualifers


Directions to import the voter registration files to MySQL:
--Export to a .csv the existing data that is in the "VoterReg_Current" table on amazon1 server/newsroomdata database. I'm storing all these archive files in a zip file (need to find a good home for them, however)
--Unzip the text files to a directory where you have lots of space
--In the newsroomdata database on amazon1 there is a saved query called "voters_importqueries" that has the import script. (A backup copy of it is stored in this github repo)
--There are 8 "load data" commands -- one for each of the text files. You'll need to check that the path listed for each script matches where you stored the 8 text files. 
--Once the .csv export of the existing data is complete, then go ahead and run the whole script. As long as all 8 text files are unzipped they should import one after another; at the end of the script are a couple update queries that populate the Address field (combining the housenumber, address and apartment number) and populate the countyname (by converting the code to a name)
--Check on the indexes and reorganize if necessary



***************************

UPDATING VOTER REG ARCHIVE AND VOTER HISTORY ARCHIVE:
The table called "voter_RegArchive" contains records from past batches of voter registration data (prior to 2015); it is currently stored as a "dump" file outside of the database. It's been culled down so that it only includes either new voters or updated records for existing voters. It looks like in the data that was put in this table prior to my time, it includes situations where the address was slightly different from one year to another. 

I've decided to not continue using that approach because the table will quickly get too burdensome to maintain. Instead, I'm going to archive off the server and store the data as text files or dump files. I think it's faster to store them as text files (in terms of exporting) and then I should be able to use the same import script (Or a slightly adjusted version) to reimport them if necessary




